---
image: "media/nnet_big.svg"
title: sentiment.ai
subtitle: Open Source AI-Based Sentiment Analysis
output:
  html_document:
    toc: yes
    toc_float:
        collapsed: true
        smooth_scroll: true
    theme: flatly
    code_folding: show
    highlight: espresso
    css: www/css/custom.css
    includes:
      in_header: header.html
  word_document:
    toc: yes
editor_options:
  chunk_output_type: console
always_allow_html: true
---

<script>
  $(document).ready(function() {
    $('#TOC').parent().prepend('<div id=\"nav_logo\"><img src=\"media/sentimentai.png"></div>');
  });
</script>



```{css, echo=FALSE}
 #TOC::before {
  font-size: 32px;
  font-weight: 900;
  text-align: center; 
  content: "sentiment.ai";
  display: block;
  width: 200px;
  height: 80px;
  line-height: 80px;
  margin: 10px 10px 10px 20px;
  //background-image: url("media/nnetsmall.png");
  background-size: contain;
  background-position: center center;
  background-repeat: no-repeat;
}
#TOC{
    border:none;
}


```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(reticulate)
require(sentiment.ai)
#sentiment.ai.init(model = "en.large")
assign("depthtrigger", 4, data.table:::.global)
```
```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
require(knitr)
require(kableExtra)
require(magrittr)
require(formattable)
require(data.table)


colLight <- "#1cb1c4"
colMed   <- "#2b8cbe"
colDark  <- "#4158CD"
colDarkest <- "#09001c"
```
<!-- [START BADGES] -->
<!-- Please keep comment here to allow auto update -->
<p align="center">
  <a href="https://github.com/BenWiseman/sentiment.ai"><img src="https://img.shields.io/github/repo-size/BenWiseman/sentiment.ai" alt="Github size" /></a>
  <a href="https://github.com/BenWiseman/sentiment.ai"><img src="https://img.shields.io/github/languages/count/BenWiseman/sentiment.ai" alt="Languages" /></a>
  <a href="https://github.com/BenWiseman/sentiment.ai"><img src="https://img.shields.io/github/languages/top/BenWiseman/sentiment.ai" alt="Languages" /></a>
  <a href="https://cran.r-project.org/"><img src="https://img.shields.io/cran/v/sentiment.ai?style=flat-square" alt="CRAN version" /></a>
  <a href="https://cran.r-project.org/"><img src="https://img.shields.io/cran/l/sentiment.ai?style=flat-square" alt="MIT License" /></a>
  <a href="https://benwiseman.github.io/sentiment.ai/"><img src="https://img.shields.io/maintenance/yes/2022" alt="MIT License" /></a>
  
</p>
<!-- [END BADGES] -->


# Overview

[Korn Ferry Institute](https://www.kornferry.com/institute)'s AITMI team made `sentiment.ai` for researchers and tinkerers who want a straight-forward way to
use powerful, open source deep learning models to improve their sentiment analyses. Our approach is relatively simple and out performs the current best offerings on CRAN and even Microsoft's Azure Cognitive Services. Given that we felt the current norm for sentiment analysis isn't quite good enough, we decided to open-source our simplified interface to turn Universal Sentence Encoder embedding vectors into sentiment scores. 

We've wrapped a lot of the underlying hassle up to make the process as simple as possible. In addition to just being cool, this approach solves several problems with traditional sentiment analysis, namely: 

1) **More robust**, can handle spelling mitsakes and mixed case, and can be applied to **diecis√©is (16) languages**! 

2) **Doesn't need a ridged lexicon**, rather it matches to an embedding vector (reduces language to a vector of numbers that capture the information, kind of like a PCA). This means you can get scores for words that are not in the lexicon but are similar to existing words! 

3) **Choose the context** for what negative and positive mean using the `sentiment_match()` function. For example, you could set `positive` to mean `"high quality"` and negative to mean `"low quality"` when looking at product reviews.

4) **Power** Because it draws from language embedding models trained on billions of texts, news articles, and wikipedia entries, it is able to detect things such as *"I learned so much on my trip to Hiroshima museum last year!"* is associated with something positive and that *"What happeded to the people of Hiroshima in 1945"* is associated with something negative.

5) **The power is yours** We've designed `sentiment.ai` such that the community can contribute sentiment models via [github](https://github.com/BenWiseman/sentiment.ai/tree/main/models). This way, it's easier for the community to work together to make sentiment analysis more reliable! 
Currently only xgboost and glms (trained on the 512-D embeddings generated with tensorflow) are supported, however in a future update we will add functionality to allow arbitrary sentiment scoring models. 


# Simple Example

```{r example1, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Load the package
require(sentiment.ai)
require(SentimentAnalysis)
require(sentimentr)

# Only if it's your first ever time
# sentiment.ai.install()

# Initiate the model
# This will create the sentiment.ai.embed model
# Do this so it can be reused without recompiling - especially on GPU!
init_sentiment.ai()

text <- c(
    "What a great car. It stopped working after a week.",
    "Steve Irwin working to save endangered species",
    "Bob Ross teaching people how to paint",
    "I saw Adolf Hitler on my vacation in Argentina...",
    "the resturant served human flesh",
    "the resturant is my favorite!",
    "the resturant is my favourite!",
    "this restront is my FAVRIT innit!",
    "the resturant was my absolute favorite until they gave me food poisoning",
    "This fantastic app freezes all the time!",
    "I learned so much on my trip to Hiroshima museum last year!",
    "What happened to the people of Hiroshima in 1945",
    "I had a blast on my trip to Nagasaki",
    "The blast in Nagasaki",
    "I love watching scary horror movies",
    "This package offers so much more nuance to sentiment analysis!",
     "you remind me of the babe. What babe? The babe with the power! What power? The power of voodoo. Who do? You do. Do what? Remind me of the babe!"
)

# sentiment.ai
sentiment.ai.score <- sentiment_score(text)

# From Sentiment Analysis
sentimentAnalysis.score <- analyzeSentiment(text)$SentimentQDAP

# From sentimentr
sentimentr.score <- sentiment_by(get_sentences(text), 1:length(text))$ave_sentiment


example <- data.table(target = text, 
                      sentiment.ai = sentiment.ai.score,
                      sentimentAnalysis = sentimentAnalysis.score,
                      sentimentr = sentimentr.score)
```


```{r draw_kable, echo = FALSE, eval=TRUE, message=FALSE}
ex_draw <- copy(example)

ex_draw$sentiment.ai %<>% round(2)
ex_draw$sentimentAnalysis %<>% round(2)
ex_draw$sentimentr %<>% round(2)


color_func <- function(x, col_high = colLight, col_low = colDarkest ){
    out <- ifelse(x > 0,
                  cell_spec(x, color = colLight, bold = TRUE),
                  ifelse(x < 0,
                         cell_spec(x, color = colDarkest, bold = TRUE),
                         cell_spec(x, color = "#808080", bold = FALSE)))
}

ex_draw[, sentiment.ai := color_func(sentiment.ai)]

ex_draw[, sentimentr := color_func(sentimentr)]

ex_draw[, sentimentAnalysis := color_func(sentimentAnalysis)]


ex_draw  %>%
 kableExtra::kable(escape = F) %>%
 kableExtra::kable_styling() %>%
 scroll_box(width = "100%", height = "600px")
```


# Benchmarks

So, what impact does more robust detection and some broader context have? To test it, in real-world scenarios,
we use two datasets/use cases:

1) classifying whether review text from Glassdoor.com is from a `pro` or a `con`

2) the popular airline tweet sentiment dataset.

We use the default settings for `sentimentr`, the QDAP dictionary in `sentimentAnalysis`, and `en.large` in `sentiment.ai`. We prefer the use of Kappa to validate classification as it's a less forgiving metric than F1 scores. In both benchmarks `sentiment.ai` comes out on top by a decent margin! 

**Note** that our testing and tuning was one using comments written in English. 

### Glassdoor

Applied example, estimating whether the text from a glassdoor.com review is positive or negative. 
The validation set used here is the same data KFI used [in our 2020 SIOP workshop](https://benwiseman.github.io/Should-You-Outsource-AI/)

Note: As a part of KFI's core purpose, `sentiment.ai`'s scoring models were tuned with extra work-related data, hence this is tilted in our favor! 

```{r benchmark_glassdoor, message=FALSE, warning=FALSE, echo=FALSE}
require(ggplot2)
# retrieve benchmark/test data 

our_scores    <- readRDS("../../docs/test/sentiment_ai_scores.rds")
vshift_scores <- readRDS("../../docs/test/sentimentr_scores.rds")
dict_scores   <- readRDS("../../docs/test/dict_scores.rds")
azure_scores  <- fread("../../docs/test/azure_scores-12084-rows.csv")

dict_scores$yhat   <- factor(round(scales::rescale(dict_scores$SentimentQDAP, to=0:1)), levels = c("0","1"))
vshift_scores$yhat <- factor(round(scales::rescale(vshift_scores$ave_sentiment, to=0:1)), levels = c("0","1"))
azure_scores$yhat  <- factor(round(scales::rescale(azure_scores$score, to=0:1)), levels = c("0","1"))
azure_scores$y     <- factor(azure_scores$target, levels = c("0", "1"))

gd_ai <- caret::confusionMatrix(data = our_scores$y_hat, reference = our_scores$y, positive = "1")
gd_sa <- caret::confusionMatrix(data = dict_scores$yhat, reference = dict_scores$y, positive = "1")
gd_sr <- caret::confusionMatrix(data = vshift_scores$yhat, reference = vshift_scores$y, positive = "1")
gd_az <- caret::confusionMatrix(data = azure_scores$yhat, reference = azure_scores$y, positive = "1")

# put into dataframe and plot!

kappas <- list(sentiment.ai      = gd_ai$overall["Kappa"],
               sentimentr        = gd_sr$overall["Kappa"],
               sentimentAnalysis = gd_sa$overall["Kappa"],
               azure             = gd_az$overall["Kappa"]) %>%
          as.data.table(keep.rownames = TRUE) %>%
          melt(measure.vars = c("sentiment.ai", "sentimentr", "sentimentAnalysis", "azure"),
               variable.name = "Method",
               value.name    = "Kappa")

setorder(kappas, Kappa)
kappas[, Method := factor(Method, levels = unique(Method))]


plot_cols <- c(sentiment.ai      = "#2b8cbeCA",
               sentimentr        = "#808080CA",
               sentimentAnalysis = "#808080CA",
               azure             = "#808080CA")

ggplot(kappas) +
  geom_bar(aes(x = Method, y = Kappa, fill = Method),
           stat = "identity") + 
  scale_fill_manual(values = plot_cols, guide = FALSE) + 
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal() + 
  theme(axis.title.x = element_blank(),
        panel.grid.minor.y = element_blank())
  
```
 

### Airline Tweets

Taken from the airline tweet dataset from Kaggle. Classification is positive vs negative 
(neutral was omitted to remove concerns about cutoff values). 

Note: Azure Cognitive Services tune their sentiment model on product reviews, as such this is tilted in favor of Azure!

```{r benchmark_airline, message=FALSE, warning=FALSE, echo=FALSE}


our_tweet_scores    <- readRDS("../../docs/test/sentimentai_tweet_scores.rds")
vshift_tweet_scores <- readRDS("../../docs/test/sentimentr_tweet_scores.rds")
dict_tweet_scores   <- readRDS("../../docs/test/dict_tweet_scores.rds")
azure_tweet_scores  <- readRDS("../../docs/test/azure_tweet_scores.rds")

dict_tweet_scores$yhat   <- factor(round(scales::rescale(dict_tweet_scores$SentimentQDAP, to=0:1)), 
                                   levels = c("0","1"))
vshift_tweet_scores$yhat <- factor(round(scales::rescale(vshift_tweet_scores$ave_sentiment, to=0:1)), 
                             levels = c("0","1"))

tw_ai <- caret::confusionMatrix(data = our_tweet_scores$y_hat, reference = our_tweet_scores$y, positive = "1")
tw_sa <- caret::confusionMatrix(data = dict_tweet_scores$yhat, reference = dict_tweet_scores$y, positive = "1")
tw_sr <- caret::confusionMatrix(data = vshift_tweet_scores$yhat, reference = vshift_tweet_scores$y, positive = "1")
tw_az <- caret::confusionMatrix(data = azure_tweet_scores$yhat, reference = azure_tweet_scores$y, positive = "1")


# put into dataframe and plot!

kappas2 <- list(sentiment.ai      = tw_ai$overall["Kappa"],
               sentimentr        = tw_sr$overall["Kappa"],
               sentimentAnalysis = tw_sa$overall["Kappa"],
               azure             = tw_az$overall["Kappa"]) %>%
          as.data.table(keep.rownames = TRUE) %>%
          melt(measure.vars = c("sentiment.ai", "sentimentr", "sentimentAnalysis", "azure"),
               variable.name = "Method",
               value.name    = "Kappa")

setorder(kappas2, Kappa)
kappas2[, Method := factor(Method, levels = unique(Method))]

ggplot(kappas2) +
  geom_bar(aes(x = Method, y = Kappa, fill = Method),
           stat = "identity") + 
  scale_fill_manual(values = plot_cols, guide = FALSE) + 
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal() + 
  theme(axis.title.x = element_blank(),
        panel.grid.minor.y = element_blank())
  
```

**Fierce.** It looks like we can be pretty confident that `sentiment.ai` is a pretty fab alternative to existing packages! Note that over time our sentiment scoring models will get better and better!

<br>
<hr> 
<br>


# Installation & Setup

## New installation

After installing `sentiment.ai` from CRAN, you will need to make sure you have a compatible python environment for `tensorflow` and `tensorflow-text`. As this can be a cumbersome experience, we included a 
convenience function to install that for you:

`install_sentiment.ai()` 

**This only needs to be run the first time you install the package**. If you're feeling adventurous, you can modify the environment it will create with the following paramaters:

* `envname` - the name of the virtual environment

* `method` - if you specifically want "conda" or "virtualenv" 

* `gpu` - set to TRUE if you want to run tensorflow-gpu

* `python_version` - The python version used in the virtual environment 

* `modules` - a names list of the dependencies and versions



```{r echo=TRUE, eval=FALSE}

# Just leave this as default unless you have a good reason to change it. 
# This is quite dependent on specific versions of python moduules
install_sentiment.ai()

```


Assuming you're using RStudio, it can be helpful to go to `tools > global options > python > python interpreter` and set your new tensorflow-ready environment as the default interpreter. There'salso an option for automatically setting project-level environments, 

**Note for GPU installations:** you'll need to make sure you have a compatible version of CUDA installed. 
(Here is a helpful guide to pick your CUDA version)[https://www.tensorflow.org/install/source#gpu]





<br>
<hr> 
<br>


## Initialize

You'll probably want to initialize the language embedding model first if you want to 1) not use the default models, and 2) want to re-apply sentiment scoring without the overhead of preparing the embedding model in memory. Pre-initializing with  `init_sentiment.ai()` is useful for making downstream sentiment scoring and matching run smoothly, especially on GPU. 

**Technically** `init_sentiment.ai()` will make an environment (`sentiment.env`) containing the `embed()` function (i.e. `sentiment.env$embed()`) which uses the pre-trained tensorflow model to embed a vector of text (also wrapped in the `embed_text()` function). Power users, the curious, and the damned may want to look in (Helper Functions for an example of this magic)[#power-move]. 


`init_sentiment.ai()` has the following optional paramaters:

### `model`

The default Universal Sentence Encoder models available are:


* `en.large` (default) use this when your text is in English and you aren't too worried about
resource use (i.e. compute time and RAM). This is the most powerful option.

* `en` use this when your text is English but your computer can't handle the larger model.

* `multi.large` use this when your text is multi lingual and you aren't too worried about
resource use (i.e. compute time and RAM)

* `multi` use this when your text is multi lingual but your computer can't handle the larger model.

* A custom tfhub URL - we try to accommodate this if you, for example, want to use an older USE model. This should work but we don't guarantee it! 


Example:

```{r echo=TRUE, eval=FALSE}

# NOTEL In most cases all you need is this:!!
init_sentiment.ai()

# To change the default behavior:
# e.g. to initialise for use on multi lingual comments:
# leave envname alone unless you have a good reason to change it! 
init_sentiment.ai(model = "multi.large")

```


### `envname`

default is `envname = "r-sentiment-ai"` - only change this if you set a different environment in `install_sentiment.ai()`


<hr>


# Sentiment Analysis

## sentiment_score()

For most use cases, `sentiment_score()` is the function you'll use. 

Returns a vector of sentiment scores. The scores are a rescaled probability of being positive (i.e 0 to 1 scaled as -1 to 1). These are calculated with a secondary scoring model which, by default is from xgboost (a simple GLM is available if for some reason xgboost doesn't work for you!).

The paramaters `sentiment_score()` takes are: 

* `x` character vector to analyse.

* `model` (optional) the name of the embedding model you are using (from `init_sentiment.ai()`)

* `scoring` (optional) the method of scoring sentiment. Options are `xgb` (default) and `glm`. `xgb` is generally more powerful, but requires xgboost (shouldn't be an issue!), `glm` is faster and almost as powerful (can be better for more 'black and white' use cases)

* `scoring_version` (optional) "1.0" is the default and (currently) only option. In future this will allow you to use updated/improved models. 

* `batch_szie` (optional) determines how many rows are processed at a time. On CPU this doesn't change much, but can be important if you installed the GPU version. Put simply, small bathes take longer but use less RAM/VRAM, large batches run faster on GPU but could exhaust memopry if too large. Default is 100 (works reliably on an RTX 2080)

Note that if `init_sentiment.ai()` has not been called before, the function will try to recover by calling it into the default environment. If you're using CUDA/GPU acceleration, you'll see a lot of work happening in the console as the model is compiled on the GPU. 

For example:
```{r, message=FALSE, warning=FALSE, error=FALSE}

my_comments <- c("Will you marry me?", "Oh, you're breaking up with me...")

# for English, default is fine
sentiment_score(my_comments)

```


## sentiment_match()

Sometimes you want to classify comments too. For that, we added `sentiment_match()` which takes a list of positive and negative terms/phrases and returs a dataframe like so: 



| Text          | Sentiment score | Phrase matched | Class of phrase matched | Similarity to phrase |
| :-----------: | :-------------: | :------------: | :---------------------: | :------------------: |
| "good stuff"  | 0.78            |"something good"| `positive`              | .30                  | 


While there are default lists of positive and negative phrases, you can overwrite them with your own. In this way you can quickly make inferences about the class of 
comments from your specific domain. The sentiment score is the same as calling `sentiment_score()` but you also get the most similar phrase, the category of that phrase, and the cosine similarity to the closest phrase. 

For example:
```{r, message=FALSE, warning=FALSE, error=FALSE}

my_comments <- c("Will you marry me?", "Oh, you're breaking up with me...")

my_positives <- c("excited", "loving", "content", "happy")
my_negatives <- c("lame", "lonely", "sad", "angry")

my_categories <- list(positive = my_positives, negative = my_negatives)

result <- sentiment_match(x = my_comments, phrases = my_categories)

print(result)

```

**Note** Cosine similarity is relative here & longer text will tend to have lower overall similarity to a specific phrase!

**Note 2** You can also be tricky and pass in a list of arbitrary themes rather than just positive and negative - in this way `sentiment.ai` can do arbitrary category matching for you! 



<hr>


# Matrix Analysis

## cosine()

A light equivilent of text2vec::sim2 that gives pairwise cosine similarity between each row of a matrix. 

```{r}

x1 <- matrix(rnorm(4 * 6), 
             ncol = 6,
             dimnames = list(c("a", "b", "c", "d"), 1:6))

x2 <- matrix(rnorm(4 * 6), 
             ncol = 6,
             dimnames = list(c("w", "x", "y", "z"), 1:6 ))

# to check that it's the same result
# all.equal(cosine(x1, x2), text2vec::sim2(x1, x2))

cosine(x1, x2)
```


## cosine_match()

This is a helper function to take two matrices, compute their `cosine` similarity, and give a pairwise ranked table. For example:

```{r}
cosine_match(target = x1, reference = x2)
```

If you filter that to only the rows where rank is 1, you'll have a table of the top matches between target and reference.

## embed_text()

As an added bonus of manually calling `init_sentiment.ai()` you can call `embed_text()` to turn a vector of text into a numeric embedding matrix (e.g if you want to cluster comments).

For example
```{r, message=FALSE, warning=FALSE}

# Note, there's some weird behavior when you do this with a single string! 
test_target <- c("dogs", "cat", "IT", "computer")
test_ref    <- c("animals", "technology")

# Work it!
target_mx <- embed_text(test_target)
ref_mx    <- embed_text(test_ref)
ref_mx[1:2, 1:4]

# And slay.
result <- cosine_match(target_mx, ref_mx)
result[rank==1] # filtered to top match (with data.table's [] syntax)
```



<br>
<hr> 
<br>


# Roadmap

We want to continue making sentiment analysis easier and better, to that end future releases will include:

* More tuned sentiment scoring models

* Sentiment scoring that gives probabilities for positive, negative, and neutral.

* Scoring models from the community (technically this *should* already be possible)

* Option for updated python dependencies/environment (requires a lot of testing)

* Python module

* Support for GPU on OSX (current limitation with tensorflow on OSX)

* Support for other embedding models (e.g. infersent) 

* Run language specific benchmarks for multilingual embedding model 

<br>
<hr> 
<br>


# Contribute a Model!

Think you can make a better sentiment scoring model? Seeing as I'm far from the sharpest lighbulb out there, you probably can out do me! Simply train a model on text embeddings and add it to the models folder of this repo. Currently we only have support for xgb and glm models (xgb worked really well with minimal faff, and GLMs, saved as simple parameter weights, are super light weight). We will figure out a way to allow custom models, likely with a custom predict script in each model folder, but that's a tomorrow-problem for now.  


The models directory contains models used to derrive sentiment after the neural nets have handles the embedding bit.

Directory structure is: model_type/version/<embedding name>.<ext>

For example: models/xgb/1.0/en.large.xgb is the default scoring model in `sentiment_score(x)` which is the same as a user passing `sentiment_score(model="en.large", scoring="xgb", scoring_version="1.0")`.When called, `sentiment.ai` will pull the scoring model from github if it hasn't done so already (default is installed during `install_sentiment.ai()`) and apply that to the embedded text. 

**NOTE**

If you'd like to contribute a new/better/context specific model, use the same folder structure
If it's not a glm or XGB, let us know so we can make it work in find_sentiment_probs()! For non-xgb/glm models, please give an example R script of applying it to a matrix of embedded text so we can add support for it. 

We only ask that numeric version (e.g xgb/2.0/...) names be left for official/default models.
For community models, the version can be a descriptor (so long as it's a valid file name and URL!)
e.g: xgb/jimbojones_imdb1/en.large.xgb
This way we can keep it easy for less engaged people who want a package that "just works"
while accommodating power users that want custom models

If you have a general purpose model (i.e. trained on a variety of sources, not context specific) that out-performs the default ones, get in touch, we'll test some extra benchmarks, and if it's "just better" we'll add it to become the new official/default (obviously giving you credit!) :)

* will add support to pass through custom github urls if you don't feel like sharing. When that's done you'd pass in: repo_url = "https://github.com/<your name>/<repo name>/raw/<branch>/models" in the ... of `sentiment_score()` or `sentiment_match()`.


NOTE FOR GLM MODELS

saving a GLM object from R uses a LOT of space. Hence we save just the parameters in a csv. Just pull the coefficients, and write.csv like so: 

`write.csv(model$coefficients, "models/glm/foo_version/en.large.csv")`


should be a text file that looks like this:

"","x"

"(Intercept)",0.922440256482062

"V1",-6.42591883182618

"V2",-3.6621793890871

...,...

"V512", 0.4204634269 


the column names don't matter, only the position.
e.g> write.csv(model$coefficients, "models/glm/2.0")


<br>
<hr> 
<br>


# Troubleshooting

Because this package requires communication between R and Python, there be dragons. We've tried to make it as seamless as possible, and absorb the pains of working on a GPU via reticulate for you, but here are a few gremlins we've encountered when trying to use Python within R. We've tested clean installs on OSX, Windows, and Ubuntu (18 & 20) and it's playing nicely, but here were issues we encountered along the way: 

### `RPyTools  is not available`

this happens on Windows sometimes. It appears to be a problem with Reticulate in RStudio. Weirdly the solution is to restart R and try the exact same thing again.

### Running on a proxy server/docker container

We've found it difficult to change environment for Reticulate in this case - try installing into the base `r-reticulate` environment. 

### Rstudio & changing python environment. 

As above. I've only encountered this a few times, I **think** it's an issue of Rstudio countermanding the python environment. You can either install in the base reticulate environment OR go to tools > global options > python and force it to use `r-sentiment.ai` (or whatever environment you want)

You may see this message if that's going to be an issue: 

 `"The RETICULATE_PYTHON environment variable is set, which can be due to being`
 `in a project (regardless of whether the global/project Python options are set),` 
 `having the global/project Python options set, or having RETICULATE_PYTHON in` 
 `your .Renviron file or bash/zsh rc files."`
                  
                  
### Error messages about missing python modules

Either something went wrong in `install_sentiment.ai()` OR Reticulate isn't changing to the `r-sentiment.ai()` environment (see points above). 

### `Error in py_run_file_impl(file, local, convert)`

If you see a message like this: 

`Preparing Model`  
`Error in py_run_file_impl(file, local, convert) :`
`AttributeError: module 'tensorflow.python.feature_column.feature_column_v2' has no attribute '_BaseFeaturesLayer'`

It may be due to a previously activated reticulate environment. e.g if you run `reticulate::py_config()` `reticulate` seems to set your environment to `r-reticulate` and won't let that change, which may mess with your tensorflow setup. Starting a new R session seems to fix that. 
 
### Using a newer version of Tensorflow

Do so at your own risk! In my experience tensorflow and its dependencies are somewhat lacking in the forwards and backwards compatibility departments having an "it's different now - deal with it" approach... If you experiment and manage get it working faster with a newer Tensorflow setup, let us know so we can update the package! If you want to use an older Tensorflow version you may be out of luck as the Universal Sentence Encoder models need the new(ish) tensorflow-text module. 

### Other errors during installation of python packages

May include incorrect tls/ssl configurations ('pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available').  Please see the following for advice on possible solutions to solve this problem, solutions may include ensuring correct path environment variables and/or resolving dll conflicts.   
  + Solutions for different operating systems : <https://stackoverflow.com/questions/45954528/pip-is-configured-with-locations-that-require-tls-ssl-however-the-ssl-module-in>
  + DLL Conflicts : <https://stackoverflow.com/questions/41328451/ssl-module-in-python-is-not-available-when-installing-package-with-pip3>


### NVIDIA CUDA missing dependencies

If running on GPU, you'll need need to make sure you have a compatible CUDA setup for tensorflow 2.4.1. For a list of compatible configurations, see <https://www.tensorflow.org/install/source#gpu>. On my personal Ubuntu 20 build, I ran into issues with `libcublas.so` - see [this comment](https://github.com/tensorflow/tensorflow/issues/26182#issuecomment-684993950) which helped me to get CUDA working properly. Interestingly, Windows has been the least hassle to get tensorflow running on GPU!

### GPU on OSX

This will be a challenge as Apple now have [their own fork of Tensorflow](https://blog.tensorflow.org/2020/11/accelerating-tensorflow-performance-on-mac.html), which we have not yet made a setup script for. If you want to use GPU acceleration on OSX, you'll need to configure your own environment, but if you do get it working, please let us know what config worked! 

### GPU Memory issues

If you get GPU errors about not being able to allocate memory, running `Sys.setenv("TF_FORCE_GPU_ALLOW_GROWTH" = "true")` after `init_sentiment.ai()` can help:


### Running inside of Rmarkdown

This is a bit unhappy, but it you call init_sentiment.ai() in the R console before knitting, it works happily. 


<br>
<hr> 
<br>


<script src="www/js/collapsible.js"></script>
